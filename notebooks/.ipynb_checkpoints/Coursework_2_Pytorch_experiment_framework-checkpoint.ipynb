{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coursework 2 - PyTorch based experiment framework\n",
    "\n",
    "## Intro\n",
    "Welcome to the coursework 2 PyTorch experiment framework. Pytorch is a  modern, intuitive, highly Pythonic and very fast framework for building differentiable graphs. Neural networks, as it happens, are a type of acyclic differentiable graph, so PyTorch is a great framework to use, if you want to build  complicated deep networks fairly easily.\n",
    "\n",
    "## MLP package vs Pytorch\n",
    "**Student**: So, why do I have to learn to use PyTorch now? I've spent all this time working on the MLP framework. Was that a waste of time?\n",
    "\n",
    "**TA**: Well, Pytorch is everything the MLP package is, but faster, cleaner and far more resourceful. Since it is one of the main deep learning frameworks being used by industry and research alike, it has been fine-tuned quite a bit, to conform to the expectation of researchers and engineers alike. What this means, is that PyTorch is now a very robust and highly flexible package, that you can use for any project you want to work on in the future, if it involves trainining differentiable graphs. \n",
    "\n",
    "In addition, MLP is written in NumPy and your time working on it has taught some low level details of NNs; PyTorch was written to look as much as possible to NumPy, so it will feel very familiar to you. The skills you have acquired are highly transferable (that is, they generalize well, so not much overfitting there...). \n",
    "\n",
    "PyTorch will almost always have all the latest and greatest implemented in it as soon as they are released as papers, and if not, give it a week or two and someone will reproduce it. If you can't wait, you can reproduce it yourself and open source it. A great way to show your skills and get github likes. \n",
    "\n",
    "In addition PyTorch has Autograd! Automatic differentiation (previously mentioned in [lectures](http://www.inf.ed.ac.uk/teaching/courses/mlp/2018-19/mlp05-learn.pdf)). \"What is this?\" you may ask. Remember having to write all those backprop functions? Forget about it. Automatic differentiation allows you to backprop through any PyTorch operation you have used in your graph, by simply calling backward(). This [blog-post](https://jdhao.github.io/2017/11/12/pytorch-computation-graph/) explains how Pytorch's autograd works at an intuitive level.\n",
    "\n",
    "**Student**: Why did we even have to use the MLP package? We did we even bother if such awesome frameworks are available?\n",
    "\n",
    "**TA**: The purpose of the MLP package was not to allow you to build fast deep learning systems. Instead, it was to help teach you the low level mechanics and sensitivities of building a deep learning system. It allowed you to delve deep into how one can go about building a deep learning framework from scratch. The intuitions you have gained from going through your assignments and courseworks allow you to see deeper in what makes or breaks a deep learning system, at a level few people actually have. You are no longer restricted to the higher level modules provided by Pytorch/TensorFlow. \n",
    "\n",
    "If, for example, a new project required you to build something that does not exist in PyTorch/TensorFlow, or otherwise modify existing modules in a way that requires understanding and intuitions on backpropagation and layer/optimizer/component implementation, you would be able to do it much more easily than others who did not. You are now equipped to understand differentiable graphs, the chain rule, numerical errors, debugging at the lowest level and deep learning system architecture. \n",
    "\n",
    "In addition, by trying to implement your modules in an efficient way, you have also become aware of how to optimize a system for efficiency, and gave you intuitions on how one could further improve such a system (parallelization of implementations). \n",
    "\n",
    "Finally, the slowness of CPU training has allowed you to understand just how important modern GPU acceleration is, for deep learning research and applications. By coming across a large breadth of problems and understanding their origins, you will now be able to both anticipate and solve future problems in a more comprehensive way than someone who did not go through the trouble of implementing the basics from scratch. \n",
    "\n",
    "**Student**: If we are switching to Pytorch, then why bother implementing convolutions in the MLP package for the coursework?\n",
    "\n",
    "**TA**: All your instructors, myself included, have found it greatly beneficial to implement convolutional networks from scratch. Once you implement convolutional layers, you will have a much deeper insight and understanding into how and why they work... as well as how they break. This way, you know what to do and what to avoid in the future. You might even be able to come with the next great network type yourself. \n",
    "\n",
    "\n",
    "## Getting Started\n",
    "\n",
    "**Student**: So, how is the learning curve of Pytorch? How do I start?\n",
    "\n",
    "**TA**: You can start by using this notebook on your experiments, it should teach you quite a lot on how to properly use PyTorch for basic conv net training. You should be aware of the [official pytorch github](https://github.com/pytorch/pytorch), the [pytorch official documentation page](https://pytorch.org/docs/stable/nn.html) and the [pytorch tutorials page](https://pytorch.org/tutorials/). \n",
    "\n",
    "Over the past year, nearly all students using PyTorch and Tensorflow on MLP and on projects found it easier and faster to get up to speed with PyTorch. In fact, I was a TensorFlow user myself, and learning TensorFlow was much more challenging than PyTorch. Mainly because TensorFlow has its own way of 'thinking' about how you build a graph and execute operations - whereas PyTorch is dynamic and works like NumPy, hence is more intuitive. If you were able to work well with the MLP package, you'll be up and running in no time. \n",
    "\n",
    "**Student**: OK, so how fast is pytorch compared to MLP?\n",
    "\n",
    "**TA**: On the CPU side of things, you'll find pytorch at least 5x faster than the MLP framework (about equal for fully connected networks, but much faster for more complicated things like convolutions - unless you write extremely efficient convolutional layer code), and if you choose to use GPUs, either using MS Azure, Google Cloud or our very own MLP Cluster (available for next semester), you can expect, depending on implementation and hardware an approximate 25-70x speed ups, compared to the CPU performance of pytorch. Yes, that means an experiment that would run overnight, now would only require about 15 minutes.\n",
    "\n",
    "**Student**: Ahh, where should I go to ask more questions?\n",
    "\n",
    "**TA**: As always, start with a Google/DuckDuckGo search, then have a look at the PyTorch Github and PyTorch docs, and if you can't find the answer come to Piazza and the lab sessions. We will be there to support you.\n",
    "\n",
    "\n",
    "#### Note: The code in this jupyter notebook is to introduce you to pytorch and allow you to play around with it in an interactive manner. However, to run your experiments, you should use the Pytorch experiment framework located in ```mlp/pytorch_experiment_scripts```. Instructions on how to use it can be found in ```notes/pytorch-experiment-framework.md``` along with the comments and documentation included in the code itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from copy import deepcopy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision\n",
    "import tqdm\n",
    "import os\n",
    "import mlp.data_providers as data_providers\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "def plot_stats_in_graph(total_losses):\n",
    "    \n",
    "    print(total_losses)\n",
    "    \n",
    "    # Plot the change in the validation and training set error over training.\n",
    "    fig_1 = plt.figure(figsize=(8, 4))\n",
    "    ax_1 = fig_1.add_subplot(111)\n",
    "    for k in total_losses.keys:\n",
    "        if \"loss\" in k:\n",
    "            ax_1.plot(np.arange(len(total_losses[k])), total_losses[k], label=k)\n",
    "    ax_1.legend(loc=0)\n",
    "    ax_1.set_xlabel('Epoch number')\n",
    "    \n",
    "    \n",
    "    fig_2 = plt.figure(figsize=(8, 4))\n",
    "    ax_2 = fig_2.add_subplot(111)\n",
    "    for k in total_losses.keys():\n",
    "        if \"acc\" in k:\n",
    "            ax_2.plot(np.arange(len(total_losses[k])), total_losses[k], label=k)\n",
    "    ax_2.legend(loc=0)\n",
    "    ax_2.set_xlabel('Epoch number')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvolutionalNetwork(nn.Module):\n",
    "    def __init__(self, input_shape, dim_reduction_type, num_output_classes, num_filters, num_layers, use_bias=False):\n",
    "        \"\"\"\n",
    "        Initializes a convolutional network module object.\n",
    "        :param input_shape: The shape of the inputs going in to the network.\n",
    "        :param dim_reduction_type: The type of dimensionality reduction to apply after each convolutional stage, should be one of ['max_pooling', 'avg_pooling', 'strided_convolution', 'dilated_convolution']\n",
    "        :param num_output_classes: The number of outputs the network should have (for classification those would be the number of classes)\n",
    "        :param num_filters: Number of filters used in every conv layer, except dim reduction stages, where those are automatically infered.\n",
    "        :param num_layers: Number of conv layers (excluding dim reduction stages)\n",
    "        :param use_bias: Whether our convolutions will use a bias.\n",
    "        \"\"\"\n",
    "        super(ConvolutionalNetwork, self).__init__()\n",
    "        # set up class attributes useful in building the network and inference\n",
    "        self.input_shape = input_shape\n",
    "        self.num_filters = num_filters\n",
    "        self.num_output_classes = num_output_classes\n",
    "        self.use_bias = use_bias\n",
    "        self.num_layers = num_layers\n",
    "        self.dim_reduction_type = dim_reduction_type\n",
    "        # initialize a module dict, which is effectively a dictionary that can collect layers and integrate them into pytorch\n",
    "        self.layer_dict = nn.ModuleDict()\n",
    "        # build the network\n",
    "        self.build_module()\n",
    "\n",
    "    def build_module(self):\n",
    "        \"\"\"\n",
    "        Builds network whilst automatically inferring shapes of layers.\n",
    "        \"\"\"\n",
    "        print(\"Building basic block of ConvolutionalNetwork using input shape\", self.input_shape)\n",
    "        x = torch.zeros((self.input_shape)) # create dummy inputs to be used to infer shapes of layers\n",
    "\n",
    "        out = x\n",
    "        # torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True)\n",
    "        for i in range(self.num_layers): # for number of layers times\n",
    "            self.layer_dict['conv_{}'.format(i)] = nn.Conv2d(in_channels=out.shape[1], # add a conv layer in the module dict\n",
    "                                                             kernel_size=3,\n",
    "                                                             out_channels=self.num_filters, padding=1,\n",
    "                                                             bias=self.use_bias)\n",
    "\n",
    "            out = self.layer_dict['conv_{}'.format(i)](out) # use layer on inputs to get an output\n",
    "            out = F.relu(out) # apply relu\n",
    "            print(out.shape)\n",
    "            if self.dim_reduction_type == 'strided_convolution': # if dim reduction is strided conv, then add a strided conv\n",
    "                self.layer_dict['dim_reduction_strided_conv_{}'.format(i)] = nn.Conv2d(in_channels=out.shape[1],\n",
    "                                                                                       kernel_size=3,\n",
    "                                                                                       out_channels=out.shape[1],\n",
    "                                                                                       padding=1,\n",
    "                                                                                       bias=self.use_bias, stride=2,\n",
    "                                                                                       dilation=1)\n",
    "\n",
    "                out = self.layer_dict['dim_reduction_strided_conv_{}'.format(i)](out) # use strided conv to get an output\n",
    "                out = F.relu(out) # apply relu to the output\n",
    "            elif self.dim_reduction_type == 'dilated_convolution': # if dim reduction is dilated conv, then add a dilated conv, using an arbitrary dilation rate of i + 2 (so it gets smaller as we go, you can choose other dilation rates should you wish to do it.)\n",
    "                self.layer_dict['dim_reduction_dilated_conv_{}'.format(i)] = nn.Conv2d(in_channels=out.shape[1],\n",
    "                                                                                       kernel_size=3,\n",
    "                                                                                       out_channels=out.shape[1],\n",
    "                                                                                       padding=1,\n",
    "                                                                                       bias=self.use_bias, stride=1,\n",
    "                                                                                       dilation=i + 2)\n",
    "                out = self.layer_dict['dim_reduction_dilated_conv_{}'.format(i)](out) # run dilated conv on input to get output\n",
    "                out = F.relu(out) # apply relu on output\n",
    "\n",
    "            elif self.dim_reduction_type == 'max_pooling':\n",
    "                self.layer_dict['dim_reduction_max_pool_{}'.format(i)] = nn.MaxPool2d(2, padding=1)\n",
    "                out = self.layer_dict['dim_reduction_max_pool_{}'.format(i)](out)\n",
    "\n",
    "            elif self.dim_reduction_type == 'avg_pooling':\n",
    "                self.layer_dict['dim_reduction_avg_pool_{}'.format(i)] = nn.AvgPool2d(2, padding=1)\n",
    "                out = self.layer_dict['dim_reduction_avg_pool_{}'.format(i)](out)\n",
    "\n",
    "            print(out.shape)\n",
    "\n",
    "        out = F.adaptive_avg_pool2d(out, 2) # apply adaptive pooling to make sure output of conv layers is always (2, 2) spacially (helps with comparisons).\n",
    "        print(out.shape)\n",
    "        self.logits_linear_layer = nn.Linear(in_features=out.view(out.shape[0], -1).shape[1],  # add a linear layer\n",
    "                                             out_features=self.num_output_classes,\n",
    "                                             bias=self.use_bias)\n",
    "        out = self.logits_linear_layer(out.view(out.shape[0], -1)) # apply linear layer on flattened inputs\n",
    "        print(\"Block is built, output volume is\", out.shape)\n",
    "        return out\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward propages the network given an input batch\n",
    "        :param x: Inputs x (b, c, h, w)\n",
    "        :return: preds (b, num_classes)\n",
    "        \"\"\"\n",
    "        out = x\n",
    "        for i in range(self.num_layers): # for number of layers\n",
    "\n",
    "            out = self.layer_dict['conv_{}'.format(i)](out) # pass through conv layer indexed at i\n",
    "            out = F.relu(out) # pass conv outputs through ReLU\n",
    "            if self.dim_reduction_type == 'strided_convolution': # if strided convolution dim reduction then\n",
    "                out = self.layer_dict['dim_reduction_strided_conv_{}'.format(i)](out) # pass previous outputs through a strided convolution indexed i\n",
    "                out = F.relu(out) # pass strided conv outputs through ReLU\n",
    "\n",
    "            elif self.dim_reduction_type == 'dilated_convolution':\n",
    "                out = self.layer_dict['dim_reduction_dilated_conv_{}'.format(i)](out)\n",
    "                out = F.relu(out)\n",
    "\n",
    "            elif self.dim_reduction_type == 'max_pooling':\n",
    "                out = self.layer_dict['dim_reduction_max_pool_{}'.format(i)](out)\n",
    "\n",
    "            elif self.dim_reduction_type == 'avg_pooling':\n",
    "                out = self.layer_dict['dim_reduction_avg_pool_{}'.format(i)](out)\n",
    "\n",
    "        out = F.adaptive_avg_pool2d(out, 2)\n",
    "        out = out.view(out.shape[0], -1) # flatten outputs from (b, c, h, w) to (b, c*h*w)\n",
    "        out = self.logits_linear_layer(out) # pass through a linear layer to get logits/preds\n",
    "        return out\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        \"\"\"\n",
    "        Re-initialize the network parameters.\n",
    "        \"\"\"\n",
    "        for item in self.layer_dict.children():\n",
    "            try:\n",
    "                item.reset_parameters()\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        self.logits_linear_layer.reset_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from mlp.pytorch_experiment_scripts.storage_utils import save_statistics\n",
    "class ExperimentBuilder(nn.Module):\n",
    "    def __init__(self, network_model, experiment_name, num_epochs, train_data, val_data,\n",
    "                 test_data, weight_decay_coefficient, use_gpu, continue_from_epoch=-1):\n",
    "        \"\"\"\n",
    "        Initializes an ExperimentBuilder object. Such an object takes care of running training and evaluation of a deep net\n",
    "        on a given dataset. It also takes care of saving per epoch models and automatically inferring the best val model\n",
    "        to be used for evaluating the test set metrics.\n",
    "        :param network_model: A pytorch nn.Module which implements a network architecture.\n",
    "        :param experiment_name: The name of the experiment. This is used mainly for keeping track of the experiment and creating and directory structure that will be used to save logs, model parameters and other.\n",
    "        :param num_epochs: Total number of epochs to run the experiment\n",
    "        :param train_data: An object of the DataProvider type. Contains the training set.\n",
    "        :param val_data: An object of the DataProvider type. Contains the val set.\n",
    "        :param test_data: An object of the DataProvider type. Contains the test set.\n",
    "        :param weight_decay_coefficient: A float indicating the weight decay to use with the adam optimizer.\n",
    "        :param use_gpu: A boolean indicating whether to use a GPU or not.\n",
    "        :param continue_from_epoch: An int indicating whether we'll start from scrach (-1) or whether we'll reload a previously saved model of epoch 'continue_from_epoch' and continue training from there.\n",
    "        \"\"\"\n",
    "        super(ExperimentBuilder, self).__init__()\n",
    "        if torch.cuda.is_available() and use_gpu: # checks whether a cuda gpu is available and whether the gpu flag is True\n",
    "            self.device = torch.device('cuda') # sets device to be cuda\n",
    "            os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\" # sets the main GPU to be the one at index 0\n",
    "            print(\"use GPU\")\n",
    "        else:\n",
    "            print(\"use CPU\")\n",
    "            self.device = torch.device('cpu') # sets the device to be CPU\n",
    "\n",
    "        self.experiment_name = experiment_name\n",
    "        self.model = network_model\n",
    "        self.model.to(self.device) # sends the model from the cpu to the gpu\n",
    "        self.model.reset_parameters() # re-initialize network parameters\n",
    "        self.train_data = train_data\n",
    "        self.val_data = val_data\n",
    "        self.test_data = test_data\n",
    "        self.optimizer = optim.Adam(self.parameters(), amsgrad=False,\n",
    "                                    weight_decay=weight_decay_coefficient)\n",
    "        # Generate the directory names\n",
    "        self.experiment_folder = os.path.abspath(experiment_name)\n",
    "        self.experiment_logs = os.path.abspath(os.path.join(self.experiment_folder, \"result_outputs\"))\n",
    "        self.experiment_saved_models = os.path.abspath(os.path.join(self.experiment_folder, \"saved_models\"))\n",
    "\n",
    "        # Set best models to be at 0 since we are just starting\n",
    "        self.best_val_model_idx = 0\n",
    "        self.best_val_model_acc = 0.\n",
    "\n",
    "        if not os.path.exists(self.experiment_folder): # If experiment directory does not exist\n",
    "            os.mkdir(self.experiment_folder) # create the experiment directory\n",
    "            os.mkdir(self.experiment_logs) # create the experiment log directory\n",
    "            os.mkdir(self.experiment_saved_models) # create the experiment saved models directory\n",
    "\n",
    "        self.num_epochs = num_epochs\n",
    "        self.criterion = nn.CrossEntropyLoss().to(self.device) # send the loss computation to the GPU\n",
    "\n",
    "        if continue_from_epoch != -1: # if continue from epoch is not -1 then\n",
    "            self.best_val_model_idx, self.best_val_model_acc = self.load_model(\n",
    "                model_save_dir=self.experiment_saved_models, model_save_name=\"train_model\",\n",
    "                model_idx=continue_from_epoch) # reload existing model from epoch and return best val model index\n",
    "                                             # and the best val acc of that model\n",
    "            self.starting_epoch = continue_from_epoch\n",
    "        else:\n",
    "            self.starting_epoch = 0\n",
    "\n",
    "    def run_train_iter(self, x, y):\n",
    "        \"\"\"\n",
    "        Receives the inputs and targets for the model and runs a training iteration. Returns loss and accuracy metrics.\n",
    "        :param x: The inputs to the model. A numpy array of shape batch_size, channels, height, width\n",
    "        :param y: The targets for the model. A numpy array of shape batch_size, num_classes\n",
    "        :return: the loss and accuracy for this batch\n",
    "        \"\"\"\n",
    "        self.train() # sets model to training mode (in case batch normalization or other methods have different procedures for training and evaluation)\n",
    "        y = np.argmax(y, axis=1) # convert one hot encoded labels to single integer labels\n",
    "        x, y = torch.Tensor(x).float().to(device=self.device), torch.Tensor(y).long().to(device=self.device) # send data to device as torch tensors\n",
    "        out = self.model.forward(x) # forward the data in the model\n",
    "        loss = F.cross_entropy(input=out, target=y) # compute loss\n",
    "\n",
    "        self.optimizer.zero_grad() # set all weight grads from previous training iters to 0\n",
    "        loss.backward() # backpropagate to compute gradients for current iter loss\n",
    "\n",
    "        self.optimizer.step() # update network parameters\n",
    "        _, predicted = torch.max(out.data, 1) # get argmax of predictions\n",
    "        accuracy = np.mean(list(predicted.eq(y.data).cpu())) # compute accuracy\n",
    "        return loss.data, accuracy\n",
    "\n",
    "    def run_evaluation_iter(self, x, y):\n",
    "        \"\"\"\n",
    "        Receives the inputs and targets for the model and runs an evaluation iterations. Returns loss and accuracy metrics.\n",
    "        :param x: The inputs to the model. A numpy array of shape batch_size, channels, height, width\n",
    "        :param y: The targets for the model. A numpy array of shape batch_size, num_classes\n",
    "        :return: the loss and accuracy for this batch\n",
    "        \"\"\"\n",
    "        self.eval() # sets the system to validation mode\n",
    "        y = np.argmax(y, axis=1) # convert one hot encoded labels to single integer labels\n",
    "        x, y = torch.Tensor(x).float().to(device=self.device), torch.Tensor(y).long().to(device=self.device) # convert data to pytorch tensors and send to the computation device\n",
    "        out = self.model.forward(x) # forward the data in the model\n",
    "        loss = F.cross_entropy(out, y) # compute loss\n",
    "        _, predicted = torch.max(out.data, 1) # get argmax of predictions\n",
    "        accuracy = np.mean(list(predicted.eq(y.data).cpu())) # compute accuracy\n",
    "        return loss.data, accuracy\n",
    "\n",
    "    def save_model(self, model_save_dir, model_save_name, model_idx, best_validation_model_idx,\n",
    "                   best_validation_model_acc):\n",
    "        \"\"\"\n",
    "        Save the network parameter state and current best val epoch idx and best val accuracy.\n",
    "        :param model_save_name: Name to use to save model without the epoch index\n",
    "        :param model_idx: The index to save the model with.\n",
    "        :param best_validation_model_idx: The index of the best validation model to be stored for future use.\n",
    "        :param best_validation_model_acc: The best validation accuracy to be stored for use at test time.\n",
    "        :param model_save_dir: The directory to store the state at.\n",
    "        :param state: The dictionary containing the system state.\n",
    "\n",
    "        \"\"\"\n",
    "        state = dict()\n",
    "        state['network'] = self.state_dict() # save network parameter and other variables.\n",
    "        state['best_val_model_idx'] = best_validation_model_idx # save current best val idx\n",
    "        state['best_val_model_acc'] = best_validation_model_acc # save current best val acc\n",
    "        torch.save(state, f=os.path.join(model_save_dir, \"{}_{}\".format(model_save_name, str(model_idx)))) # save state at prespecified filepath\n",
    "\n",
    "    def load_model(self, model_save_dir, model_save_name, model_idx):\n",
    "        \"\"\"\n",
    "        Load the network parameter state and the best val model idx and best val acc to be compared with the future val accuracies, in order to choose the best val model\n",
    "        :param model_save_dir: The directory to store the state at.\n",
    "        :param model_save_name: Name to use to save model without the epoch index\n",
    "        :param model_idx: The index to save the model with.\n",
    "        :return: best val idx and best val model acc, also it loads the network state into the system state without returning it\n",
    "        \"\"\"\n",
    "        state = torch.load(f=os.path.join(model_save_dir, \"{}_{}\".format(model_save_name, str(model_idx))))\n",
    "        self.load_state_dict(state_dict=state['network'])\n",
    "        return state['best_val_model_idx'], state['best_val_model_acc']\n",
    "\n",
    "    def run_experiment(self):\n",
    "        \"\"\"\n",
    "        Runs experiment train and evaluation iterations, saving the model and best val model and val model accuracy after each epoch\n",
    "        :return: The summary current_epoch_losses from starting epoch to total_epochs.\n",
    "        \"\"\"\n",
    "        total_losses = {\"train_acc\": [], \"train_loss\": [], \"val_acc\": [], \"val_loss\": []} # initialize a dict to keep the per-epoch metrics\n",
    "        for i, epoch_idx in enumerate(range(self.starting_epoch, self.num_epochs)):\n",
    "            epoch_start_time = time.time()\n",
    "            current_epoch_losses = {\"train_acc\": [], \"train_loss\": [], \"val_acc\": [], \"val_loss\": []}\n",
    "\n",
    "            with tqdm.tqdm(total=self.train_data.num_batches) as pbar_train: # create a progress bar for training\n",
    "                for idx, (x, y) in enumerate(self.train_data): # get data batches\n",
    "                    loss, accuracy = self.run_train_iter(x=x, y=y) # take a training iter step\n",
    "                    current_epoch_losses[\"train_loss\"].append(loss) # add current iter loss to the train loss list\n",
    "                    current_epoch_losses[\"train_acc\"].append(accuracy) # add current iter acc to the train acc list\n",
    "                    pbar_train.update(1)\n",
    "                    pbar_train.set_description(\"loss: {:.4f}, accuracy: {:.4f}\".format(loss, accuracy))\n",
    "\n",
    "            with tqdm.tqdm(total=self.val_data.num_batches) as pbar_val: # create a progress bar for validation\n",
    "                for x, y in self.val_data: # get data batches\n",
    "                    loss, accuracy = self.run_evaluation_iter(x=x, y=y) # run a validation iter\n",
    "                    current_epoch_losses[\"val_loss\"].append(loss) # add current iter loss to val loss list.\n",
    "                    current_epoch_losses[\"val_acc\"].append(accuracy) # add current iter acc to val acc lst.\n",
    "                    pbar_val.update(1) # add 1 step to the progress bar\n",
    "                    pbar_val.set_description(\"loss: {:.4f}, accuracy: {:.4f}\".format(loss, accuracy))\n",
    "\n",
    "            if np.mean(current_epoch_losses['val_acc']) > self.best_val_model_acc: # if current epoch's mean val acc is greater than the saved best val acc then\n",
    "                self.best_val_model_acc = np.mean(current_epoch_losses['val_acc']) # set the best val model acc to be current epoch's val accuracy\n",
    "                self.best_val_model_idx = epoch_idx # set the experiment-wise best val idx to be the current epoch's idx\n",
    "\n",
    "            for key, value in current_epoch_losses.items():\n",
    "                total_losses[key].append(np.mean(value)) # get mean of all metrics of current epoch metrics dict, to get them ready for storage and output on the terminal.\n",
    "\n",
    "            save_statistics(experiment_log_dir=self.experiment_logs, filename='summary.csv',\n",
    "                            stats_dict=total_losses, current_epoch=i) # save statistics to stats file.\n",
    "\n",
    "            # load_statistics(experiment_log_dir=self.experiment_logs, filename='summary.csv') # How to load a csv file if you need to\n",
    "\n",
    "            out_string = \"_\".join([\"{}_{:.4f}\".format(key, np.mean(value)) for key, value in current_epoch_losses.items()])\n",
    "            # create a string to use to report our epoch metrics\n",
    "            epoch_elapsed_time = time.time() - epoch_start_time # calculate time taken for epoch\n",
    "            epoch_elapsed_time = \"{:.4f}\".format(epoch_elapsed_time)\n",
    "            print(\"Epoch {}:\".format(epoch_idx), out_string, \"epoch time\", epoch_elapsed_time, \"seconds\")\n",
    "            self.save_model(model_save_dir=self.experiment_saved_models, # save model and best val idx and best val acc, using the model dir, model name and model idx\n",
    "                            model_save_name=\"train_model\", model_idx=epoch_idx,\n",
    "                            best_validation_model_idx=self.best_val_model_idx,\n",
    "                            best_validation_model_acc=self.best_val_model_acc)\n",
    "\n",
    "        print(\"Generating test set evaluation metrics\")\n",
    "        self.load_model(model_save_dir=self.experiment_saved_models, model_idx=self.best_val_model_idx, # load best validation model\n",
    "                        model_save_name=\"train_model\")\n",
    "        current_epoch_losses = {\"test_acc\": [], \"test_loss\": []} # initialize a statistics dict\n",
    "        with tqdm.tqdm(total=self.test_data.num_batches) as pbar_test: # ini a progress bar\n",
    "            for x, y in self.test_data: # sample batch\n",
    "                loss, accuracy = self.run_evaluation_iter(x=x, y=y) # compute loss and accuracy by running an evaluation step\n",
    "                current_epoch_losses[\"test_loss\"].append(loss) # save test loss\n",
    "                current_epoch_losses[\"test_acc\"].append(accuracy) # save test accuracy\n",
    "                pbar_test.update(1) # update progress bar status\n",
    "                pbar_test.set_description(\"loss: {:.4f}, accuracy: {:.4f}\".format(loss, accuracy))  # update progress bar string output\n",
    "\n",
    "        test_losses = {key: [np.mean(value)] for key, value in current_epoch_losses.items()} # save test set metrics in dict format\n",
    "        save_statistics(experiment_log_dir=self.experiment_logs, filename='test_summary.csv', # save test set metrics on disk in .csv format\n",
    "                        stats_dict=test_losses, current_epoch=0)\n",
    "\n",
    "        return total_losses, test_losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KeysView(<numpy.lib.npyio.NpzFile object at 0x7f0f2786ed30>)\n",
      "KeysView(<numpy.lib.npyio.NpzFile object at 0x7f0f2786edd8>)\n",
      "KeysView(<numpy.lib.npyio.NpzFile object at 0x7f0f2786ee10>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 3.8518, accuracy: 0.0000:  10%|█         | 1/10 [00:00<00:00, 10.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building basic block of ConvolutionalNetwork using input shape (100, 1, 28, 28)\n",
      "torch.Size([100, 32, 28, 28])\n",
      "torch.Size([100, 32, 15, 15])\n",
      "torch.Size([100, 32, 15, 15])\n",
      "torch.Size([100, 32, 8, 8])\n",
      "torch.Size([100, 32, 8, 8])\n",
      "torch.Size([100, 32, 5, 5])\n",
      "torch.Size([100, 32, 5, 5])\n",
      "torch.Size([100, 32, 3, 3])\n",
      "torch.Size([100, 32, 2, 2])\n",
      "Block is built, output volume is torch.Size([100, 47])\n",
      "use CPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 3.8460, accuracy: 0.0200: 100%|██████████| 10/10 [00:01<00:00,  9.39it/s]\n",
      "loss: 3.8555, accuracy: 0.0000: 100%|██████████| 158/158 [00:06<00:00, 24.37it/s]\n",
      "loss: 3.8479, accuracy: 0.0100:  20%|██        | 2/10 [00:00<00:00, 10.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: train_acc_0.0170_train_loss_3.8489_val_acc_0.0309_val_loss_3.8485 epoch time 7.7674 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 3.8460, accuracy: 0.0200: 100%|██████████| 10/10 [00:01<00:00,  9.21it/s]\n",
      "loss: 3.8483, accuracy: 0.0100: 100%|██████████| 158/158 [00:05<00:00, 27.14it/s]\n",
      "loss: 3.8440, accuracy: 0.0000:  10%|█         | 1/10 [00:00<00:00,  9.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train_acc_0.0260_train_loss_3.8481_val_acc_0.0223_val_loss_3.8441 epoch time 7.0674 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 3.8436, accuracy: 0.0600: 100%|██████████| 10/10 [00:00<00:00,  9.73it/s]\n",
      "loss: 3.8470, accuracy: 0.0000: 100%|██████████| 158/158 [00:05<00:00, 26.82it/s]\n",
      "loss: 3.8335, accuracy: 0.0600:  20%|██        | 2/10 [00:00<00:00, 11.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: train_acc_0.0340_train_loss_3.8396_val_acc_0.0511_val_loss_3.8292 epoch time 7.0545 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 3.7772, accuracy: 0.0300: 100%|██████████| 10/10 [00:01<00:00,  9.31it/s]\n",
      "loss: 3.7680, accuracy: 0.0300: 100%|██████████| 158/158 [00:05<00:00, 26.65it/s]\n",
      "loss: 3.7963, accuracy: 0.0200:  20%|██        | 2/10 [00:00<00:00, 10.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: train_acc_0.0550_train_loss_3.8100_val_acc_0.0362_val_loss_3.7835 epoch time 7.1235 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 3.6729, accuracy: 0.1000: 100%|██████████| 10/10 [00:01<00:00,  8.31it/s]\n",
      "loss: 3.5945, accuracy: 0.1000: 100%|██████████| 158/158 [00:06<00:00, 25.25it/s]\n",
      "loss: 3.6113, accuracy: 0.1300:   3%|▎         | 5/158 [00:00<00:05, 26.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: train_acc_0.0680_train_loss_3.7397_val_acc_0.1075_val_loss_3.6357 epoch time 7.4930 seconds\n",
      "Generating test set evaluation metrics\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 3.6532, accuracy: 0.0800: 100%|██████████| 158/158 [00:06<00:00, 25.29it/s]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 100\n",
    "image_num_channels = 1\n",
    "image_height = 28\n",
    "image_width = 28\n",
    "dim_reduction_idx = 2\n",
    "dim_reduction_type = ['strided_convolution', 'dilated_convolution', 'max_pooling', 'avg_pooling']\n",
    "num_filters = 32\n",
    "num_layers = 4\n",
    "experiment_name = 'example_experiment_1'\n",
    "num_epochs = 5\n",
    "weight_decay_coefficient = 1e-05\n",
    "seed = 9112018\n",
    "use_gpu = False\n",
    "\n",
    "rng = np.random.RandomState(seed=seed)\n",
    "train_data = data_providers.EMNISTDataProvider('train', batch_size=batch_size, rng=rng, max_num_batches=10)\n",
    "val_data = data_providers.EMNISTDataProvider('valid', batch_size=batch_size, rng=rng)\n",
    "test_data = data_providers.EMNISTDataProvider('test', batch_size=batch_size, rng=rng)\n",
    "\n",
    "\n",
    "custom_conv_net = ConvolutionalNetwork(\n",
    "    input_shape=(batch_size, image_num_channels, image_height, image_width),\n",
    "    dim_reduction_type=dim_reduction_type[dim_reduction_idx],\n",
    "    num_output_classes=47, num_filters=num_filters, num_layers=num_layers, use_bias=False)\n",
    "\n",
    "conv_experiment = ExperimentBuilder(network_model=custom_conv_net,\n",
    "                                    experiment_name=experiment_name,\n",
    "                                    num_epochs=num_epochs,\n",
    "                                    weight_decay_coefficient=weight_decay_coefficient,\n",
    "                                    use_gpu=use_gpu,\n",
    "                                    train_data=train_data, val_data=val_data, test_data=test_data)\n",
    "total_losses = conv_experiment.run_experiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'train_acc': [0.017, 0.026000000000000002, 0.034, 0.05500000000000001, 0.068], 'train_loss': [3.8488603, 3.848102, 3.8395774, 3.810017, 3.7396877], 'val_acc': [0.030886075949367084, 0.02234177215189873, 0.05113924050632911, 0.036202531645569615, 0.10753164556962022], 'val_loss': [3.8484504, 3.8441343, 3.8291733, 3.783537, 3.6357203]}, {'test_acc': [0.10784810126582278], 'test_loss': [3.6382625]})\n",
      "({'train_acc': [0.017, 0.026000000000000002, 0.034, 0.05500000000000001, 0.068], 'train_loss': [3.8488603, 3.848102, 3.8395774, 3.810017, 3.7396877], 'val_acc': [0.030886075949367084, 0.02234177215189873, 0.05113924050632911, 0.036202531645569615, 0.10753164556962022], 'val_loss': [3.8484504, 3.8441343, 3.8291733, 3.783537, 3.6357203]}, {'test_acc': [0.10784810126582278], 'test_loss': [3.6382625]})\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'keys'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-----------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-4f9ee779b84d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_losses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mplot_stats_in_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_losses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-21-7dc159f1b97a>\u001b[0m in \u001b[0;36mplot_stats_in_graph\u001b[0;34m(total_losses)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mfig_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0max_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfig_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_subplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m111\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtotal_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"loss\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0max_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_losses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_losses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'keys'"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_stats_in_graph(total_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
